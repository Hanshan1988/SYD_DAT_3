{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science with Python \n",
    "## General Assembly - 22nd February 2016 Day 1##\n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "This notebook contains exercises for getting started with Natual Language Processing in Python. The main topics we will cover in this class are:\n",
    "1. Tokenizing\n",
    "2. Stemming\n",
    "3. Speech Tagging\n",
    "4. Named Entity Recognition\n",
    "5. Term Frequency - Inverse Document Frequency\n",
    "6. Latent Dirichlet Allocation\n",
    "\n",
    "Other:\n",
    "- Analysing data with the Alchemy API\n",
    "- Further Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "\n",
    "Why:   Gives structure to previously unstructured text\n",
    "\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "\n",
    "\"corpus\" = collection of documents\n",
    "\n",
    "\"corpora\" = plural form of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining, and predictive analytics, similar to Knowledge Discovery in Databases (KDD).   Data science employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information science, and computer sc'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\")\n",
    "b = BeautifulSoup(r.text)\n",
    "paragraphs = b.find(\"body\").findAll(\"p\")\n",
    "text = \"\"\n",
    "for paragraph in paragraphs:\n",
    "    text += paragraph.text + \" \"\n",
    "# Data Science corpus\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining, and predictive analytics, similar to Knowledge Discovery in Databases (KDD).',\n",
       " u'Data science employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information science, and computer science, including signal processing, probability models, machine learning, statistical learning, data mining, database, data engineering, pattern recognition and learning, visualization, predictive analytics, uncertainty modeling, data warehousing, data compression, computer programming, artificial intelligence, and high performance computing.',\n",
       " u'Methods that scale to big data are of particular interest in data science, although the discipline is not generally considered to be restricted to such big data, and big data solutions are often focused on organizing and preprocessing the data instead of analysis.',\n",
       " u'The development of machine learning has enhanced the growth and importance of data science.',\n",
       " u'Data science affects academic and applied research in many domains, including machine translation, speech recognition, robotics, search engines, digital economy, but also the biological sciences, medical informatics, health care, social sciences and the humanities.',\n",
       " u'It heavily influences economics, business and finance.',\n",
       " u'From the business perspective, data science is an integral part of competitive intelligence, a newly emerging field that encompasses a number of activities, such as data mining and data analysis.',\n",
       " u'[3] Data scientists use their data and analytical ability to find and interpret rich data sources; manage large amounts of data despite hardware, software, and bandwidth constraints; merge data sources; ensure consistency of datasets; create visualizations to aid in understanding data; build mathematical models using the data; and present and communicate the data insights/findings.',\n",
       " u'They are often expected to produce answers in days rather than months, work by exploratory analysis and rapid iteration, and to get/present results with dashboards (displays of current values) rather than papers/reports, as statisticians normally do.',\n",
       " u'[4] \"Data Scientist\" has become a popular occupation with Harvard Business Review dubbing it \"The Sexiest Job of the 21st Century\" [5] and McKinsey & Company projecting a global excess demand of 1.5 million new data scientists.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into sentences\n",
    "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data',\n",
       " u'Science',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinary',\n",
       " u'field',\n",
       " u'about',\n",
       " u'processes',\n",
       " u'and',\n",
       " u'systems',\n",
       " u'to',\n",
       " u'extract',\n",
       " u'knowledge',\n",
       " u'or',\n",
       " u'insights',\n",
       " u'from',\n",
       " u'data',\n",
       " u'in',\n",
       " u'various',\n",
       " u'forms',\n",
       " u',',\n",
       " u'either',\n",
       " u'structured',\n",
       " u'or',\n",
       " u'unstructured',\n",
       " u',',\n",
       " u'[',\n",
       " u'1',\n",
       " u']',\n",
       " u'[',\n",
       " u'2',\n",
       " u']',\n",
       " u'which',\n",
       " u'is',\n",
       " u'a',\n",
       " u'continuation',\n",
       " u'of',\n",
       " u'some',\n",
       " u'of',\n",
       " u'the',\n",
       " u'data',\n",
       " u'analysis',\n",
       " u'fields',\n",
       " u'such',\n",
       " u'as',\n",
       " u'statistics',\n",
       " u',',\n",
       " u'data',\n",
       " u'mining',\n",
       " u',',\n",
       " u'and',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u',',\n",
       " u'similar',\n",
       " u'to',\n",
       " u'Knowledge',\n",
       " u'Discovery',\n",
       " u'in',\n",
       " u'Databases',\n",
       " u'(',\n",
       " u'KDD',\n",
       " u')',\n",
       " u'.',\n",
       " u'Data',\n",
       " u'science',\n",
       " u'employs',\n",
       " u'techniques',\n",
       " u'and',\n",
       " u'theories',\n",
       " u'drawn',\n",
       " u'from',\n",
       " u'many',\n",
       " u'fields',\n",
       " u'within',\n",
       " u'the',\n",
       " u'broad',\n",
       " u'areas',\n",
       " u'of',\n",
       " u'mathematics',\n",
       " u',',\n",
       " u'statistics',\n",
       " u',',\n",
       " u'information',\n",
       " u'science',\n",
       " u',',\n",
       " u'and',\n",
       " u'computer',\n",
       " u'science',\n",
       " u',',\n",
       " u'including',\n",
       " u'signal',\n",
       " u'processing',\n",
       " u',',\n",
       " u'probability',\n",
       " u'models',\n",
       " u',',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u',']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into words\n",
    "tokens = [word for word in nltk.word_tokenize(text)]\n",
    "tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data',\n",
       " u'Science',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinary',\n",
       " u'field',\n",
       " u'about',\n",
       " u'processes',\n",
       " u'and',\n",
       " u'systems',\n",
       " u'to',\n",
       " u'extract',\n",
       " u'knowledge',\n",
       " u'or',\n",
       " u'insights',\n",
       " u'from',\n",
       " u'data',\n",
       " u'in',\n",
       " u'various',\n",
       " u'forms',\n",
       " u'either',\n",
       " u'structured',\n",
       " u'or',\n",
       " u'unstructured',\n",
       " u'which',\n",
       " u'is',\n",
       " u'a',\n",
       " u'continuation',\n",
       " u'of',\n",
       " u'some',\n",
       " u'of',\n",
       " u'the',\n",
       " u'data',\n",
       " u'analysis',\n",
       " u'fields',\n",
       " u'such',\n",
       " u'as',\n",
       " u'statistics',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'and',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u'similar',\n",
       " u'to',\n",
       " u'Knowledge',\n",
       " u'Discovery',\n",
       " u'in',\n",
       " u'Databases',\n",
       " u'KDD',\n",
       " u'Data',\n",
       " u'science',\n",
       " u'employs',\n",
       " u'techniques',\n",
       " u'and',\n",
       " u'theories',\n",
       " u'drawn',\n",
       " u'from',\n",
       " u'many',\n",
       " u'fields',\n",
       " u'within',\n",
       " u'the',\n",
       " u'broad',\n",
       " u'areas',\n",
       " u'of',\n",
       " u'mathematics',\n",
       " u'statistics',\n",
       " u'information',\n",
       " u'science',\n",
       " u'and',\n",
       " u'computer',\n",
       " u'science',\n",
       " u'including',\n",
       " u'signal',\n",
       " u'processing',\n",
       " u'probability',\n",
       " u'models',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'statistical',\n",
       " u'learning',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'database',\n",
       " u'data',\n",
       " u'engineering',\n",
       " u'pattern',\n",
       " u'recognition',\n",
       " u'and',\n",
       " u'learning',\n",
       " u'visualization',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u'uncertainty',\n",
       " u'modeling',\n",
       " u'data',\n",
       " u'warehousing',\n",
       " u'data',\n",
       " u'compression',\n",
       " u'computer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep tokens that start with a letter (using regular expressions)\n",
    "import re\n",
    "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
    "clean_tokens[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 48),\n",
       " (u'data', 46),\n",
       " (u'the', 44),\n",
       " (u'of', 39),\n",
       " (u'science', 20),\n",
       " (u'in', 20),\n",
       " (u'to', 17),\n",
       " (u'a', 16),\n",
       " (u'Data', 14),\n",
       " (u'In', 11),\n",
       " (u'for', 11),\n",
       " (u'as', 10),\n",
       " (u'is', 10),\n",
       " (u'Science', 10),\n",
       " (u'term', 8),\n",
       " (u'his', 7),\n",
       " (u'The', 6),\n",
       " (u'analysis', 6),\n",
       " (u'scientists', 5),\n",
       " (u'such', 5),\n",
       " (u'their', 5),\n",
       " (u'that', 5),\n",
       " (u'are', 5),\n",
       " (u'used', 5),\n",
       " (u'has', 5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the tokens\n",
    "from collections import Counter\n",
    "c = Counter(clean_tokens)\n",
    "\n",
    "c.most_common(25)       # mixed case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 1\n",
      "Although 1\n",
      "American 1\n",
      "An 1\n",
      "April 2\n",
      "Areas 1\n",
      "Association 1\n",
      "Board 1\n",
      "Business 1\n",
      "C. 1\n",
      "C.F 1\n",
      "CODATA 1\n",
      "Carver 1\n",
      "Century 2\n",
      "Chandra 1\n",
      "Classification 1\n",
      "Cleveland 2\n",
      "Collections 1\n",
      "Columbia 1\n",
      "Committee 1\n",
      "Company 1\n",
      "Computer 1\n",
      "Concise 1\n",
      "Council 1\n",
      "DJ 1\n"
     ]
    }
   ],
   "source": [
    "sorted(c.items())[:25]  # counts similar words separately\n",
    "for item in sorted(c.items())[:25]:\n",
    "    print item[0], item[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "##### EXERCISE ####\n",
    "###################\n",
    "\n",
    "# Put each word in clean_tokens in lower case\n",
    "# find the new word count of the lowered tokens\n",
    "# Then show the top 10 words used in this corpus\n",
    "clean_tokens_lower = [w.lower() for w in clean_tokens]\n",
    "\n",
    "c = Counter(clean_tokens_lower)\n",
    "c.most_common(10)       # mixed case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "\n",
    "Notes: Uses a \"simple\" and fast rule-based approach\n",
    "       Output can be undesirable for irregular words\n",
    "       Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "       Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x1ab932e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charg\n",
      "charg\n",
      "charg\n"
     ]
    }
   ],
   "source": [
    "# example stemming\n",
    "print(stemmer.stem('charge'))\n",
    "print(stemmer.stem('charging'))\n",
    "print(stemmer.stem('charged'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'data',\n",
       " u'scienc',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinari',\n",
       " u'field',\n",
       " u'about',\n",
       " u'process',\n",
       " u'and',\n",
       " u'system']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem the tokens\n",
    "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
    "stemmed_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'data', 60),\n",
       " (u'the', 50),\n",
       " (u'and', 48),\n",
       " (u'of', 39),\n",
       " (u'scienc', 32),\n",
       " (u'in', 31),\n",
       " (u'to', 17),\n",
       " (u'a', 16),\n",
       " (u'statist', 15),\n",
       " (u'for', 11),\n",
       " (u'is', 10),\n",
       " (u'as', 10),\n",
       " (u'scientist', 9),\n",
       " (u'use', 8),\n",
       " (u'term', 8),\n",
       " (u'comput', 8),\n",
       " (u'his', 7),\n",
       " (u'field', 7),\n",
       " (u'method', 6),\n",
       " (u'analysi', 6),\n",
       " (u'such', 5),\n",
       " (u'their', 5),\n",
       " (u'that', 5),\n",
       " (u'model', 5),\n",
       " (u'are', 5)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the stemmed tokens\n",
    "c = Counter(stemmed_tokens)\n",
    "c.most_common(25)       # all lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 16),\n",
       " (u'abil', 1),\n",
       " (u'about', 1),\n",
       " (u'academ', 2),\n",
       " (u'action', 1),\n",
       " (u'activ', 2),\n",
       " (u'address', 1),\n",
       " (u'advanc', 1),\n",
       " (u'advoc', 1),\n",
       " (u'affect', 1),\n",
       " (u'aid', 1),\n",
       " (u'all', 1),\n",
       " (u'also', 1),\n",
       " (u'although', 2),\n",
       " (u'american', 1),\n",
       " (u'amount', 1),\n",
       " (u'an', 5),\n",
       " (u'analysi', 6),\n",
       " (u'analyt', 3),\n",
       " (u'analytics\\u201d', 1),\n",
       " (u'and', 48),\n",
       " (u'annot', 1),\n",
       " (u'answer', 1),\n",
       " (u'appli', 2),\n",
       " (u'applic', 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(c.items())[:25]  # some are strange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lemmatization\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "    \n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "    \n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# compare stemmer to lemmatizer\n",
    "print(stemmer.stem('dogs'))\n",
    "print(lemmatizer.lemmatize('dogs'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolv\n",
      "wolf\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('wolves')) # Beter for information retrieval and search\n",
    "print(lemmatizer.lemmatize('wolves')) # Better for text analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "is\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('is'))\n",
    "print(lemmatizer.lemmatize('is'))\n",
    "print(lemmatizer.lemmatize('is',pos='v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part of Speech Tagging\n",
    "\n",
    "What:  Determine the part of speech of a word\n",
    "    \n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "    \n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sinan', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Patrick', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Liam', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('great', 'JJ'),\n",
       " ('teachers', 'NNS'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_sent = 'Sinan and Patrick and Liam are great teachers!'\n",
    "# pos_tag takes a tokenize sentence\n",
    "nltk.pos_tag(nltk.word_tokenize(temp_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ian', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Alasdair', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('teaching', 'VBG'),\n",
       " ('at', 'IN'),\n",
       " ('General', 'NNP'),\n",
       " ('Assembly', 'NNP'),\n",
       " ('this', 'DT'),\n",
       " ('Monday', 'NNP'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_sent = 'Ian and Alasdair are teaching at General Assembly this Monday!'\n",
    "# pos_tag takes a tokenize sentence\n",
    "nltk.pos_tag(nltk.word_tokenize(temp_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stopword Removal\n",
    "\n",
    "What:  Remove common words that will likely appear in any text\n",
    "    \n",
    "Why:   They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'data', 60),\n",
       " (u'the', 50),\n",
       " (u'and', 48),\n",
       " (u'of', 39),\n",
       " (u'scienc', 32),\n",
       " (u'in', 31),\n",
       " (u'to', 17),\n",
       " (u'a', 16),\n",
       " (u'statist', 15),\n",
       " (u'for', 11),\n",
       " (u'is', 10),\n",
       " (u'as', 10),\n",
       " (u'scientist', 9),\n",
       " (u'use', 8),\n",
       " (u'term', 8),\n",
       " (u'comput', 8),\n",
       " (u'his', 7),\n",
       " (u'field', 7),\n",
       " (u'method', 6),\n",
       " (u'analysi', 6),\n",
       " (u'such', 5),\n",
       " (u'their', 5),\n",
       " (u'that', 5),\n",
       " (u'model', 5),\n",
       " (u'are', 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'all',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'as',\n",
       " u'at',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'did',\n",
       " u'do',\n",
       " u'does',\n",
       " u'doing',\n",
       " u'don',\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'few',\n",
       " u'for',\n",
       " u'from',\n",
       " u'further',\n",
       " u'had',\n",
       " u'has',\n",
       " u'have',\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'me',\n",
       " u'more',\n",
       " u'most',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'once',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u's',\n",
       " u'same',\n",
       " u'she',\n",
       " u'should',\n",
       " u'so',\n",
       " u'some',\n",
       " u'such',\n",
       " u't',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u'very',\n",
       " u'was',\n",
       " u'we',\n",
       " u'were',\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'data', 60),\n",
       " (u'scienc', 32),\n",
       " (u'statist', 15),\n",
       " (u'scientist', 9),\n",
       " (u'use', 8),\n",
       " (u'term', 8),\n",
       " (u'comput', 8),\n",
       " (u'field', 7),\n",
       " (u'method', 6),\n",
       " (u'analysi', 6),\n",
       " (u'model', 5),\n",
       " (u'statistician', 5),\n",
       " (u'lectur', 5),\n",
       " (u'learn', 4),\n",
       " (u'publish', 4),\n",
       " (u'busi', 4),\n",
       " (u'digit', 3),\n",
       " (u'process', 3),\n",
       " (u'research', 3),\n",
       " (u'mine', 3),\n",
       " (u'analyt', 3),\n",
       " (u'databas', 3),\n",
       " (u'mani', 3),\n",
       " (u'present', 3),\n",
       " (u'journal', 3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "### Exercise  ####\n",
    "##################\n",
    "\n",
    "\n",
    "# Create a variable called stemmed_stops which is the \n",
    "# stemmed version of each stopword in stopwords\n",
    "# Use the stemmer we used up above!\n",
    "stopwords_stemmed = [stemmer.stem(t) for t in stopwords]\n",
    "\n",
    "# Then create a list called stemmed_tokens_no_stop that \n",
    "# contains only the tokens in stemmed_tokens that aren't in \n",
    "# stemmed_stops\n",
    "stemmed_tokens_no_stop = [t for t in stemmed_tokens if t not in stopwords_stemmed]\n",
    "\n",
    "# Show the 25 most common stemmed non stop word tokens\n",
    "c = Counter(stemmed_tokens_no_stop)\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Named Entity Recognition\n",
    "\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "\n",
    "Why:   Can help you to identify \"important\" words\n",
    "\n",
    "Notes: Training NER classifier requires a lot of annotated training data\n",
    "       Should be trained on data relevant to your task\n",
    "       Stanford NER classifier is the \"gold standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ian', 'is', 'an', 'instructor', 'for', 'General', 'Assembly']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Ian is an instructor for General Assembly'\n",
    "\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ian', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('instructor', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('General', 'NNP'),\n",
       " ('Assembly', 'NNP')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\hansh\\Anaconda2\\lib\\site-packages\\IPython\\core\\formatters.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_get_formatter_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hansh\\Anaconda2\\lib\\site-packages\\nltk\\tree.pyc\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[0;32m    727\u001b[0m                             \u001b[1;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[1;32mC:\\Users\\hansh\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    533\u001b[0m                 binary_names=None, url=None, verbose=True):\n\u001b[0;32m    534\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[1;32m--> 535\u001b[1;33m                                  binary_names, url, verbose))\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[1;32mC:\\Users\\hansh\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \"\"\"\n\u001b[0;32m    528\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[1;32m--> 529\u001b[1;33m                      url, verbose):\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hansh\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    498\u001b[0m                         (filename, url))\n\u001b[0;32m    499\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Ian', 'NNP')]), ('is', 'VBZ'), ('an', 'DT'), ('instructor', 'NN'), ('for', 'IN'), Tree('ORGANIZATION', [('General', 'NNP'), ('Assembly', 'NNP')])])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(tagged)\n",
    "\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPE] Ian\n",
      "[ORGANIZATION] General Assembly\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Ian is an instructor for General Assembly'):\n",
    "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "How: \n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bob', u'hates', u'likes', u'sports', u'trees']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row represents a sentence\n",
    "# Each column represents a word\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bob', u'hates', u'likes', u'sports', u'trees']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'bob': 1.0, u'trees': 1.6931471805599454, u'likes': 1.2876820724517808, u'hates': 1.6931471805599454, u'sports': 1.2876820724517808}\n"
     ]
    }
   ],
   "source": [
    "# the IDF of each word\n",
    "idf = tfidf.idf_\n",
    "print dict(zip(tfidf.get_feature_names(), idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "## Exercise ###\n",
    "###############\n",
    "\n",
    "d = dict(zip(tfidf.get_feature_names(), idf))\n",
    "\n",
    "# for each sentence in sample, find the most \"interesting \n",
    "#words\" by ordering their tfidf in ascending order\n",
    "sample_lower = [t.lower() for t in sample]\n",
    "\n",
    "for item in sample_lower:\n",
    "    miw = item[0]\n",
    "    for word in item.split(\" \")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###LDA - Latent Dirichlet Allocation\n",
    "\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "\n",
    "Why:   Much quicker than manually creating and identifying topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named lda",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-8be4fc5f024d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Instantiate a count vectorizer with two additional parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msentences_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named lda"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLE: Automatically summarize a document\n",
    "\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TFIDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print '\\nLOWEST:\\n'\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print '\\nHIGHEST:\\n'\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TextBlob Demo: \"Simplified Text Processing\"\n",
    "# Installation: pip install textblob\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identify words and noun phrases\n",
    "blob = TextBlob('Liam and Sinan are instructors for General Assembly')\n",
    "blob.words\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print TextBlob('Ian and Alasdair are instructors for General Assembly in Sydney').sentiment.subjectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[word.pluralize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "blob.translate(to='es')\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
